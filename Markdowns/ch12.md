# Intermediate 12 : 다나와에서 나의 관심 목록 가져오기
***
1. 우선 다나와 로그인 코드를 다시한번 작성해 봅시다
```python3
import os
from selenium import webdriver
from selenium.webdriver.common.by import By

danawa_url = 'http://www.danawa.com/'

id = ""
pw = ""

#Get Driver
driver = webdriver.Chrome(f"{os.getcwd()}/chromedriver")
driver.implicitly_wait(5)
#Request to danawa
driver.get(danawa_url)

driver.find_element(By.CLASS_NAME,'btn_user--login').click()
driver.implicitly_wait(3)
#Send ID
driver.find_element(By.ID,'danawa-member-login-input-id').send_keys(id)
#Send Key
driver.find_element(By.ID,'danawa-member-login-input-pwd').send_keys(pw)
#Find Login Button and log in
driver.find_element(By.ID,'danawa-member-login-loginButton').click()
driver.implicitly_wait(10)
```

2. 다나와에서 아무 상품이나 검색한 후에 관심상품에 등록해 주자

3. 다나와 관심상품 들어가는 코드 작성해 보기

![image](https://user-images.githubusercontent.com/45956041/147798049-dcda84c2-b0bb-4489-b4e0-a8e1953cd6f4.png)

  - 이 부분에 대해 class name으로 element지정 후 click을 하겠습니다. 그 다음 관심목록 페이지 소스(html)를 반환한 다음 크롬 드라이버를 종료해줍니다
  - 페이지 소스는 driver객체의 page_source속성에 저장됩니다.
```python
driver.find_element(By.CLASS_NAME,'btn_user--wish').click()
page_html = driver.page_source
driver.close()
```
4. Beautiful Soup를 이용해서 관심목록 스크레이핑 / 정리
이제 페이지 소스를 불러왔으므로, 불러온 페이지 소스를 BeautifulSoup를 사용해서 스크레이핑 해줍니다. 총 출력 두개를 내보는거로 하겠습니다. 하나는 JSON으로 정리, 하나는 콘솔에 출력해봅시다.우선적으로 페이지 소스는 저장되어있으므로, 따로 requests를 이용해서 요청을 보낼 필요가 없음을 알 수 있습니다.
![image](https://user-images.githubusercontent.com/45956041/147798546-05fef730-8d19-4e7a-ab4f-915f6480b5bb.png)
- 우선적으로 wish_tbl이라는 class를 가진 table 태그의 자식 요소 tbody요소에 상품들에 대한 정보가 있는것을 알 수 있습니다.그리고 각 tr태그에 각 상품별 정보가 들어있는것을 볼 수 있습니다
![image](https://user-images.githubusercontent.com/45956041/147798797-8a75b955-3f98-4924-a6ab-d13fe4fb3483.png)
그리고 각 tr태그 안에 td에 정보가 있는데, class : check는 체크박스 여부, img : 상품 이미지, info : 제품 설명, lowest : 최저가 정보가 담겨있는것을 볼 수 있습니다.제가 작성한 코드는 아래와 같습니다.
코드와 결과는 이 [링크](https://github.com/J-hoplin1/Dummy-Codies/blob/master/usefulpython/ch12)에서도 보실수 있습니다.
```python
import os,json
from selenium import webdriver
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup

danawa_url = 'http://www.danawa.com/'

id = ""
pw = ""

#Get Driver
driver = webdriver.Chrome(f"{os.getcwd()}/chromedriver")
driver.implicitly_wait(5)
#Request to danawa
driver.get(danawa_url)

driver.find_element(By.CLASS_NAME,'btn_user--login').click()
driver.implicitly_wait(3)
#Send ID
driver.find_element(By.ID,'danawa-member-login-input-id').send_keys(id)
#Send Key
driver.find_element(By.ID,'danawa-member-login-input-pwd').send_keys(pw)
#Find Login Button and log in
driver.find_element(By.ID,'danawa-member-login-loginButton').click()
driver.implicitly_wait(10)

driver.find_element(By.CLASS_NAME,'btn_user--wish').click()
page_html = driver.page_source
driver.close()

json_element = dict()

soup_instance = BeautifulSoup(page_html,'html.parser')
get_element_per_items = soup_instance.find("table",attrs={'class' : 'wish_tbl'}).find('tbody').find_all('tr')

os.system('clear')

for j,i in enumerate(get_element_per_items,start=1):
    img_link = i.find('td',{'class' : 'img'}).find('a').get('href')
    item_title = i.find('td',{'class' : 'info'}).find('div',{'class' : 'tit'}).text
    item_spec = i.find('td',{'class' : 'info'}).find('dd',{'class' : 'spec_list'}).find('a').text
    item_cost = i.find('td',{'class' : 'lowest'}).find('span',{'class' : 'price'}).find('em').text + "원"
    json_element[f"Item {j}"] = {
        "Image URL" : f"{img_link}",
        "Item Title" : item_title,
        "Item Spec" : item_spec,
        "Item Cost" : item_cost
    }
    statement = f'''
    {"=" * 40}
    Item title : {item_title}
    Item Spec : {item_spec}
    Item Cost : {item_cost}
    {"=" * 40}
    '''
    print(statement)

with open("My_WishList.json",'w') as f:
    #ensure ascii : 한글은 기본적으로 16진수로 표기되어 저장된다. 한글로 저장하기 위해서는 ensure_ascii를 false로 바꿔줘야한다.
    json.dump(json_element,f,indent=4,ensure_ascii=False)


print("Json saved!")
```

  
